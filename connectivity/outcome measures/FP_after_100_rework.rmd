---
title: "over 100 sketch"
author: "Mikkel Werling"
date: "5/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#set working directory
setwd("~/CRUST-1/data/new_data")

#packages 
library(pacman)
p_load(tidyverse, ggunchained) 
```

loading data. 

```{r}
#reading 
path <- "~/CRUST-1/data/tests/"

files <- list.files(path = path, pattern="*.csv") #if possible. 

mycols <- c("replica", 
            "network", 
            "population", 
            "sigma",
            "true_model",
            "turn",
            "studies",
            "net_size",
            "base_SS",
            "colab_cond",
            "prop_1",
            "prop_2",
            "prop_3",
            "prop_4",
            "prop_5",
            "prop_6",
            "prop_7",
            "prop_8",
            "prop_9",
            "prop_10",
            "prop_11",
            "prop_12",
            "prop_13",
            "prop_14",
            "prop_true")

#loading them 
dfList <- lapply(files, function(f) {  
   read.csv(paste0(path, f))[mycols]
})

#subsetting (because of computational demands)
df <- do.call(rbind, dfList) 
rm(dfList)
```

Overview of variables:

```{r}
var <- df %>%
  group_by(network, population, sigma, net_size, colab_cond,
           base_SS, true_model) %>%
  summarize(n())
```

```{r}
what_bo <- df %>%
  filter(network == "TOM", colab_cond == "yes", population == "Bo")

summary(what_bo)
```



Hit/Miss 50

```{r}
#clearing & reading 

##CHANGING NET_SIZE INTO A FACTOR FOR LAST PART OF THE CODE - SHOULD BE DONE IN THE PREPROCESSING
df$net_size <- as.factor(df$net_size)
df$true_model <- as.factor(df$true_model)

#SETTING A CONSTANT REPLICA - SEE IF THAT WORKS
replica_con <- max(df$replica)
true_model_con <- length(levels(df$true_model))

#how many of them got there
hit_miss_50 <- df %>%
  group_by(replica, population, network, sigma, true_model, net_size, base_SS, colab_cond) %>%
  summarize(hit50 = ifelse(max(prop_true >= 0.5), 1, 0))

#percentage of runs got to 50
hit_miss_50_sum <- hit_miss_50 %>%
  group_by(population, network, sigma, true_model, net_size, base_SS, colab_cond) %>%
  summarize(mean = mean(hit50))

#tallying up how many of misses and hits to 50%
hit_miss_50_tally <- hit_miss_50 %>%
  group_by(population, network, sigma, true_model, net_size, base_SS, hit50, colab_cond) %>%
  summarize(count = n())

#first time to true model at 50%
df_100_fftm <- df %>%
  group_by(replica, population, network, sigma, true_model, net_size, base_SS, colab_cond) %>%
  mutate(hit50 = ifelse(max(prop_true >= 0.5), 1, 0)) %>%
  filter(hit50 == 1) %>%
  summarize(first_hit50 = min(which(prop_true >= 0.5)))

#summary of first time to to 50%
df_100_fftm_summary_averaged <- df_100_fftm %>% 
  group_by(population, network, net_size, colab_cond) %>%
  summarise(FP50_median = median(first_hit50),
            FP50_mean = mean(first_hit50),
            FP50_IQR = IQR(first_hit50),
            FP50_misses = (replica_con * true_model_con) - n())

#MISSES DOESN'T WORK - MAYBE SOMETHING TO DO WITH BO MAN


##NOT NECESSARY TO ADD BO ATM

#adding bo
#df_100_fftm_summary_averaged[4,] <- df_100_fftm_summary_averaged[2,]
#df_100_fftm_summary_averaged[2,] <- c("Bo", "Small", NA, NA, NA, 450)
```

Most adhered to 

```{r}

#Which rows have the true model as the most adhered to?

df_100_fttma <- df %>%
  mutate(most_adhered = ifelse(15 == max.col(.[,c(11:25)], ties.method = "last"), 1, 0)) 

#Filtering so only those runs that have the true model as the most adhered to are included 
df_100_filter_100 <- df_100_fttma %>%
  group_by(replica, population, network, sigma, true_model, net_size, base_SS, colab_cond) %>%
  filter(mean(most_adhered) > 0, studies >= 100)

#new col for which model is the most adhered to at the time
df_100_filter_100$max_col <- max.col(df_100_filter_100[, c(11:25)], "last")

#when is the true model first most adhered to
df_100_filter_final <- df_100_filter_100 %>%
  group_by(replica, population, network, sigma, true_model, net_size, base_SS, colab_cond) %>%
  summarize(min_studies = min(studies), fpttt1 = (min_studies-1) + min(which(max_col == 15))) %>%
  filter(fpttt1 != Inf)

#summarizing the above
df_100_filter_final_sum <- df_100_filter_final %>%
  group_by(population, network, net_size, colab_cond) %>%
  summarize(FPMA_median = median(fpttt1), 
            FPMA_mean = mean(fpttt1),
            FPMA_IQR = IQR(fpttt1),
            FPMA_misses = (max(df$replica) * length(levels(df$network)) * length(levels(df$net_size))) - n()) #Made flexible, so that the number of misses accurately depict how many possibilities there are. If we need to group over more stuff, they should also be added. 


```

MAIN EFFECTS:

```{r}
colab_final_sum <- df_100_filter_final %>%
  group_by(colab_cond) %>%
  summarize(FPMA_median = median(fpttt1), 
            FPMA_mean = mean(fpttt1),
            FPMA_IQR = IQR(fpttt1)) 

network_final_sum <- df_100_filter_final %>%
  group_by(network) %>%
  summarize(FPMA_median = median(fpttt1), 
            FPMA_mean = mean(fpttt1),
            FPMA_IQR = IQR(fpttt1)) 

population_final_sum <- df_100_filter_final %>%
  group_by(population) %>%
  summarize(FPMA_median = median(fpttt1), 
            FPMA_mean = mean(fpttt1),
            FPMA_IQR = IQR(fpttt1))

sigma_final_sum <- df_100_filter_final %>%
  group_by(sigma) %>%
  summarize(FPMA_median = median(fpttt1), 
            FPMA_mean = mean(fpttt1),
            FPMA_IQR = IQR(fpttt1))

true_final_sum <- df_100_filter_final %>%
  group_by(true_model) %>%
  summarize(FPMA_median = median(fpttt1), 
            FPMA_mean = mean(fpttt1),
            FPMA_IQR = IQR(fpttt1))

```

Main effects:
network: Lattice and Small are similar. When this is said, small has a bit larger median, but a smaller mean with a larger IQR. TOM is doing worse than the two networks, with a larger median, a larger mean, and a large IQR. This also suggests that TOM in some cases is a very fast network, but highly unstable. 

pop: Quite similar to our previous results. Bo is the worst, the highest mean and the largest IQR. All is second in all aspects, except for the mean where Tess is 2nd. Mave has the smallest mean with the smallest IQR, but Tess has the smallest median. 

sigma: although the median is smaller for 0.2 sigma, the mean is actually smaller with a lower IQR. How do we explain this? 

colab: The median of “no” and “yes” is fairly similar (110, 116), but the mean and the IQR differ. For YES conditions, we have a much higher mean and larger IQR.
tmod: for tmod = 2, we have lower median, mean and IQR. 

INTERACTION WITH NETWORK:

```{r}
colab_network_sum <- df_100_filter_final %>%
  group_by(colab_cond, network) %>%
  summarize(FPMA_median = median(fpttt1), 
            FPMA_mean = mean(fpttt1),
            FPMA_IQR = IQR(fpttt1)) 

population_network_sum <- df_100_filter_final %>%
  group_by(population, network) %>%
  summarize(FPMA_median = median(fpttt1), 
            FPMA_mean = mean(fpttt1),
            FPMA_IQR = IQR(fpttt1))

sigma_network_sum <- df_100_filter_final %>%
  group_by(sigma, network) %>%
  summarize(FPMA_median = median(fpttt1), 
            FPMA_mean = mean(fpttt1),
            FPMA_IQR = IQR(fpttt1))

tmod_network_sum <- df_100_filter_final %>%
  group_by(true_model, network) %>%
  summarize(FPMA_median = median(fpttt1), 
            FPMA_mean = mean(fpttt1),
            FPMA_IQR = IQR(fpttt1))


```

Interactions with network:
colab: For lattice, we see almost no difference. But especially for TOM, colab has a strong interaction effect. Here we see a much larger median, mean and IQR. This suggests that in many cases, colab slow down the progress of these networks. This is quite trivial, as the colabs in TOM use a lot more studies. 

pop: In contrast to All & Mave, who become significantly slower in TOM, Bo has her lowest mean in the TOM-network, and lowest median in Small. Tess has a quite similar median but a much higher mean in TOM. 
How do we explain this?
Could be due to the fact that Bo relies heavily on her neighbors to find the true model. A more well connected network will increase the chances of Bo finding the true model. 


sigma: The overall picture is that medians increase when sigma = 0.8. However, there are some interesting cases here:
The mean for lattice decreases drastically with sigma 0.8 and a much lower IQR.


Writing files

```{r}

write.csv(df_100_fftm_summary_averaged, "C:/Users/Mikkel/Documents/CRUST-1/connectivity/outcome_results/FP50_small_100.csv", row.names = F)

write.csv(df_100_filter_final_sum, "C:/Users/Mikkel/Documents/CRUST-1/connectivity/outcome_results/FPMA_small_100.csv", row.names = F)

```
